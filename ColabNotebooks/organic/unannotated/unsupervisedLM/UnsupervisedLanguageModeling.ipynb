{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UnsupervisedLanguageModeling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LaerCOwlVEyY","colab_type":"code","outputId":"48418b47-c082-4582-a27a-83245644cd79","executionInfo":{"status":"ok","timestamp":1560520347637,"user_tz":-120,"elapsed":16639,"user":{"displayName":"Farrukh Mushtaq","photoUrl":"https://lh5.googleusercontent.com/-81ZvUwaF1xk/AAAAAAAAAAI/AAAAAAAAAIU/7V7zhrncda4/s64/photo.jpg","userId":"05056985930475718301"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["!pip install pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 36.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n","Collecting regex (from pytorch-pretrained-bert)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n","\u001b[K     |████████████████████████████████| 655kB 41.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.165)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.165 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.165)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (0.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (2.5.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (1.12.0)\n","Building wheels for collected packages: regex\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n","Successfully built regex\n","Installing collected packages: regex, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.6.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QxpZIKYyVcR1","colab_type":"code","outputId":"e2956608-04d8-4b27-bf07-ad0aaac27eb6","executionInfo":{"status":"ok","timestamp":1560520368192,"user_tz":-120,"elapsed":35292,"user":{"displayName":"Farrukh Mushtaq","photoUrl":"https://lh5.googleusercontent.com/-81ZvUwaF1xk/AAAAAAAAAAI/AAAAAAAAAIU/7V7zhrncda4/s64/photo.jpg","userId":"05056985930475718301"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xLqeRvE7V1S2","colab_type":"code","colab":{}},"source":["sharedGFolderPath = \"/content/gdrive/My Drive/TUM/NLP Lab/Colab Notebooks/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSsg3QJEYYLI","colab_type":"code","colab":{}},"source":["from argparse import Namespace\n","from pathlib import Path\n","import torch\n","import logging\n","import json\n","import random\n","import numpy as np\n","from collections import namedtuple\n","from tempfile import TemporaryDirectory\n","\n","from torch.utils.data import DataLoader, Dataset, RandomSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm\n","\n","from pytorch_pretrained_bert.modeling import BertForPreTraining\n","from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbSNh6t2VA2t","colab_type":"code","colab":{}},"source":["InputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask segment_ids lm_label_ids is_next\")\n","\n","log_format = '%(asctime)-10s: %(message)s'\n","logging.basicConfig(level=logging.INFO, format=log_format)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ge8OwuR1VA2w","colab_type":"code","colab":{}},"source":["def convert_example_to_features(example, tokenizer, max_seq_length):\n","    tokens = example[\"tokens\"]\n","    segment_ids = example[\"segment_ids\"]\n","    is_random_next = example[\"is_random_next\"]\n","    masked_lm_positions = example[\"masked_lm_positions\"]\n","    masked_lm_labels = example[\"masked_lm_labels\"]\n","\n","    assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n","\n","    input_array = np.zeros(max_seq_length, dtype=np.int)\n","    input_array[:len(input_ids)] = input_ids\n","\n","    mask_array = np.zeros(max_seq_length, dtype=np.bool)\n","    mask_array[:len(input_ids)] = 1\n","\n","    segment_array = np.zeros(max_seq_length, dtype=np.bool)\n","    segment_array[:len(segment_ids)] = segment_ids\n","\n","    lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-1)\n","    lm_label_array[masked_lm_positions] = masked_label_ids\n","\n","    features = InputFeatures(input_ids=input_array,\n","                             input_mask=mask_array,\n","                             segment_ids=segment_array,\n","                             lm_label_ids=lm_label_array,\n","                             is_next=is_random_next)\n","    return features\n","\n","\n","class PregeneratedDataset(Dataset):\n","    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n","        self.vocab = tokenizer.vocab\n","        self.tokenizer = tokenizer\n","        self.epoch = epoch\n","        self.data_epoch = epoch % num_data_epochs\n","        data_file = training_path / f\"epoch_{self.data_epoch}.json\"\n","        metrics_file = training_path / f\"epoch_{self.data_epoch}_metrics.json\"\n","        assert data_file.is_file() and metrics_file.is_file()\n","        metrics = json.loads(metrics_file.read_text())\n","        num_samples = metrics['num_training_examples']\n","        seq_len = metrics['max_seq_len']\n","        self.temp_dir = None\n","        self.working_dir = None\n","        if reduce_memory:\n","            self.temp_dir = TemporaryDirectory()\n","            self.working_dir = Path(self.temp_dir.name)\n","            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n","                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n","            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n","                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n","            segment_ids = np.memmap(filename=self.working_dir/'segment_ids.memmap',\n","                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n","            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n","                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n","            lm_label_ids[:] = -1\n","            is_nexts = np.memmap(filename=self.working_dir/'is_nexts.memmap',\n","                                 shape=(num_samples,), mode='w+', dtype=np.bool)\n","        else:\n","            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n","            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n","            segment_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n","            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=-1)\n","            is_nexts = np.zeros(shape=(num_samples,), dtype=np.bool)\n","        logging.info(f\"Loading training examples for epoch {epoch}\")\n","        with data_file.open() as f:\n","            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n","                line = line.strip()\n","                example = json.loads(line)\n","                features = convert_example_to_features(example, tokenizer, seq_len)\n","                input_ids[i] = features.input_ids\n","                segment_ids[i] = features.segment_ids\n","                input_masks[i] = features.input_mask\n","                lm_label_ids[i] = features.lm_label_ids\n","                is_nexts[i] = features.is_next\n","        assert i == num_samples - 1  # Assert that the sample count metric was true\n","        logging.info(\"Loading complete!\")\n","        self.num_samples = num_samples\n","        self.seq_len = seq_len\n","        self.input_ids = input_ids\n","        self.input_masks = input_masks\n","        self.segment_ids = segment_ids\n","        self.lm_label_ids = lm_label_ids\n","        self.is_nexts = is_nexts\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, item):\n","        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n","                torch.tensor(self.input_masks[item].astype(np.int64)),\n","                torch.tensor(self.segment_ids[item].astype(np.int64)),\n","                torch.tensor(self.lm_label_ids[item].astype(np.int64)),\n","                torch.tensor(self.is_nexts[item].astype(np.int64)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hh76VYY6VA2y","colab_type":"code","colab":{}},"source":["def readConfig():\n","    args = Namespace()\n","    args.pregenerated_data = Path(sharedGFolderPath+'organic/unannotated/unsupervisedLM/data/V02/')\n","    args.bert_model = 'bert-base-uncased'\n","    args.output_dir = Path(sharedGFolderPath+'organic/unannotated/unsupervisedLM/Model/bert-uncased-pretrained-organic-V03/')\n","    args.reduce_memory = False\n","    args.do_lower_case = True\n","    args.train_batch_size = 32\n","    args.learning_rate = 3e-5\n","    args.epochs = 3\n","    args.warmup_proportion = 0.1\n","    args.no_cuda = False\n","    args.local_rank = -1\n","    args.seed = 42\n","    args.gradient_accumulation_steps = 1\n","    args.fp16 = False\n","    args.loss_scale = 0\n","    \n","    return args"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FXyh5zKVA20","colab_type":"code","colab":{}},"source":["def unsupervisedLM():\n","    args = readConfig()\n","\n","    assert args.pregenerated_data.is_dir(), \\\n","        \"--pregenerated_data should point to the folder of files made by pregenerate_training_data.py!\"\n","\n","    samples_per_epoch = []\n","    for i in range(args.epochs):\n","        epoch_file = args.pregenerated_data / f\"epoch_{i}.json\"\n","        metrics_file = args.pregenerated_data / f\"epoch_{i}_metrics.json\"\n","        if epoch_file.is_file() and metrics_file.is_file():\n","            metrics = json.loads(metrics_file.read_text())\n","            samples_per_epoch.append(metrics['num_training_examples'])\n","        else:\n","            if i == 0:\n","                exit(\"No training data was found!\")\n","            print(f\"Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({args.epochs}).\")\n","            print(\"This script will loop over the available data, but training diversity may be negatively impacted.\")\n","            num_data_epochs = i\n","            break\n","    else:\n","        num_data_epochs = args.epochs\n","\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        n_gpu = torch.cuda.device_count()\n","    else:\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        n_gpu = 1\n","        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.distributed.init_process_group(backend='nccl')\n","    logging.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n","        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n","                            args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","    if args.output_dir.is_dir() and list(args.output_dir.iterdir()):\n","        logging.warning(f\"Output directory ({args.output_dir}) already exists and is not empty!\")\n","    args.output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n","\n","    total_train_examples = 0\n","    for i in range(args.epochs):\n","        # The modulo takes into account the fact that we may loop over limited epochs of data\n","        total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n","\n","    num_train_optimization_steps = int(\n","        total_train_examples / args.train_batch_size / args.gradient_accumulation_steps)\n","    if args.local_rank != -1:\n","        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n","\n","    # Prepare model\n","    model = BertForPreTraining.from_pretrained(args.bert_model)\n","    if args.fp16:\n","        model.half()\n","    model.to(device)\n","    if args.local_rank != -1:\n","        try:\n","            from apex.parallel import DistributedDataParallel as DDP\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","        model = DDP(model)\n","    elif n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Prepare optimizer\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","\n","    if args.fp16:\n","        try:\n","            from apex.optimizers import FP16_Optimizer\n","            from apex.optimizers import FusedAdam\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","        optimizer = FusedAdam(optimizer_grouped_parameters,\n","                              lr=args.learning_rate,\n","                              bias_correction=False,\n","                              max_grad_norm=1.0)\n","        if args.loss_scale == 0:\n","            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n","        else:\n","            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n","        warmup_linear = WarmupLinearSchedule(warmup=args.warmup_proportion,\n","                                             t_total=num_train_optimization_steps)\n","    else:\n","        optimizer = BertAdam(optimizer_grouped_parameters,\n","                             lr=args.learning_rate,\n","                             warmup=args.warmup_proportion,\n","                             t_total=num_train_optimization_steps)\n","\n","    global_step = 0\n","    logging.info(\"***** Running training *****\")\n","    logging.info(f\"  Num examples = {total_train_examples}\")\n","    logging.info(\"  Batch size = %d\", args.train_batch_size)\n","    logging.info(\"  Num steps = %d\", num_train_optimization_steps)\n","    model.train()\n","    traceLoss = []\n","    for epoch in range(args.epochs):\n","        epoch_dataset = PregeneratedDataset(epoch=epoch, training_path=args.pregenerated_data, tokenizer=tokenizer,\n","                                            num_data_epochs=num_data_epochs, reduce_memory=args.reduce_memory)\n","        if args.local_rank == -1:\n","            train_sampler = RandomSampler(epoch_dataset)\n","        else:\n","            train_sampler = DistributedSampler(epoch_dataset)\n","        train_dataloader = DataLoader(epoch_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\") as pbar:\n","            for step, batch in enumerate(train_dataloader):\n","                batch = tuple(t.to(device) for t in batch)\n","                input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\n","                loss = model(input_ids, segment_ids, input_mask, lm_label_ids, is_next)\n","                if n_gpu > 1:\n","                    loss = loss.mean() # mean() to average on multi-gpu.\n","                if args.gradient_accumulation_steps > 1:\n","                    loss = loss / args.gradient_accumulation_steps\n","                if args.fp16:\n","                    optimizer.backward(loss)\n","                else:\n","                    loss.backward()\n","                tr_loss += loss.item()\n","                nb_tr_examples += input_ids.size(0)\n","                nb_tr_steps += 1\n","                pbar.update(1)\n","                mean_loss = tr_loss * args.gradient_accumulation_steps / nb_tr_steps\n","                traceLoss.append(mean_loss)\n","                pbar.set_postfix_str(f\"Loss: {mean_loss:.5f}\")\n","                if (step + 1) % args.gradient_accumulation_steps == 0:\n","                    if args.fp16:\n","                        # modify learning rate with special warm up BERT uses\n","                        # if args.fp16 is False, BertAdam is used that handles this automatically\n","                        lr_this_step = args.learning_rate * warmup_linear.get_lr(global_step, args.warmup_proportion)\n","                        for param_group in optimizer.param_groups:\n","                            param_group['lr'] = lr_this_step\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    global_step += 1\n","\n","    # Save a trained model\n","    logging.info(\"** ** * Saving fine-tuned model ** ** * \")\n","    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","    output_model_file = args.output_dir / \"pytorch_model.bin\"\n","    torch.save(model_to_save.state_dict(), str(output_model_file))\n","    tokenizer.save_vocabulary(args.output_dir)\n","    \n","    # Return loss trace\n","    return traceLoss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7KdK8bMVA22","colab_type":"code","outputId":"be54cf5a-65be-4365-900f-52a9849d5d04","executionInfo":{"status":"ok","timestamp":1560528079188,"user_tz":-120,"elapsed":7672044,"user":{"displayName":"Farrukh Mushtaq","photoUrl":"https://lh5.googleusercontent.com/-81ZvUwaF1xk/AAAAAAAAAAI/AAAAAAAAAIU/7V7zhrncda4/s64/photo.jpg","userId":"05056985930475718301"}},"colab":{"base_uri":"https://localhost:8080/","height":819}},"source":["lossTrace = unsupervisedLM()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["2019-06-14 13:53:32,304: device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n","2019-06-14 13:53:33,579: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpxdcdnpkp\n","100%|██████████| 231508/231508 [00:00<00:00, 352689.66B/s]\n","2019-06-14 13:53:35,143: copying /tmp/tmpxdcdnpkp to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2019-06-14 13:53:35,144: creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2019-06-14 13:53:35,146: removing temp file /tmp/tmpxdcdnpkp\n","2019-06-14 13:53:35,147: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","2019-06-14 13:53:36,133: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpswsztynz\n","100%|██████████| 407873900/407873900 [00:34<00:00, 11672217.06B/s]\n","2019-06-14 13:54:12,023: copying /tmp/tmpswsztynz to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","2019-06-14 13:54:13,375: creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","2019-06-14 13:54:13,376: removing temp file /tmp/tmpswsztynz\n","2019-06-14 13:54:13,435: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","2019-06-14 13:54:13,436: extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpj5zc5sok\n","2019-06-14 13:54:17,971: Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","2019-06-14 13:54:28,504: ***** Running training *****\n","2019-06-14 13:54:28,506:   Num examples = 235786\n","2019-06-14 13:54:28,507:   Batch size = 32\n","2019-06-14 13:54:28,514:   Num steps = 7368\n","2019-06-14 13:54:28,551: Loading training examples for epoch 0\n","Training examples: 100%|██████████| 78761/78761 [00:09<00:00, 7917.31it/s] \n","2019-06-14 13:54:38,508: Loading complete!\n","Epoch 0: 100%|██████████| 2462/2462 [42:06<00:00,  1.20it/s, Loss: 2.91719]\n","2019-06-14 14:36:45,251: Loading training examples for epoch 1\n","Training examples: 100%|██████████| 78675/78675 [00:09<00:00, 8321.76it/s] \n","2019-06-14 14:36:54,714: Loading complete!\n","Epoch 1: 100%|██████████| 2459/2459 [42:12<00:00,  1.10it/s, Loss: 2.65520]\n","2019-06-14 15:19:07,072: Loading training examples for epoch 2\n","Training examples: 100%|██████████| 78350/78350 [00:09<00:00, 8444.90it/s] \n","2019-06-14 15:19:16,359: Loading complete!\n","Epoch 2: 100%|██████████| 2449/2449 [42:00<00:00,  1.15it/s, Loss: 2.57941]2019-06-14 16:01:16,485: Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n","\n","2019-06-14 16:01:16,575: ** ** * Saving fine-tuned model ** ** * \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8oKcdggmYqQ4","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKQjVMUtY-Zd","colab_type":"code","colab":{}},"source":["def running_mean(x, N):\n","    cumsum = np.cumsum(np.insert(x, 0, 0)) \n","    return (cumsum[N:] - cumsum[:-N]) / float(N)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebFa7Rb_3Skh","colab_type":"code","outputId":"395c1745-d9e5-4942-fef7-42bd9cda6802","executionInfo":{"status":"ok","timestamp":1560528185539,"user_tz":-120,"elapsed":833,"user":{"displayName":"Farrukh Mushtaq","photoUrl":"https://lh5.googleusercontent.com/-81ZvUwaF1xk/AAAAAAAAAAI/AAAAAAAAAIU/7V7zhrncda4/s64/photo.jpg","userId":"05056985930475718301"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["plt.plot(running_mean(lossTrace,50),label='Training Loss')\n","plt.legend()\n","plt.show()"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFO4iIjlI4S4ikoP+Py0kCH9t5F4OAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"phEk6tYZYXXJ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}